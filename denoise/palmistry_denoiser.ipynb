{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNFbtmQ/UgVOTI19T1caJti"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Setup"],"metadata":{"id":"B8NwaEpcNwBU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MtuExGO1NtjR"},"outputs":[],"source":["from google.colab import drive\n","from os.path import join\n","ROOT = \"/content/drive\"\n","drive.mount(ROOT)"]},{"cell_type":"code","source":["%cd \"/content/drive/MyDrive/study/Github/palmistry/denoise\""],"metadata":{"id":"dlNJ9vrmNz45"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","DEVICE = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n","print(DEVICE)"],"metadata":{"id":"-crxut3YN5zB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Dataset"],"metadata":{"id":"-CSWgwxUN8Gr"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","from pathlib import Path\n","from os import listdir\n","import logging\n","from os.path import splitext\n","from PIL import Image\n","import numpy as np"],"metadata":{"id":"KXc2RBbWN9By"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BasicDataset(Dataset):\n","    def __init__(self, images_dir: str, masks_dir: str, size: int, mask_suffix: str = '', transform=None):\n","        self.images_dir = Path(images_dir)\n","        self.masks_dir = Path(masks_dir)\n","        self.size = size\n","        self.mask_suffix = mask_suffix\n","        self.transform = transform\n","\n","        self.ids = [splitext(file)[0] for file in listdir(images_dir) if not file.startswith('.')]\n","        if not self.ids:\n","            raise RuntimeError(f'No input file found in {images_dir}, make sure you put your images there')\n","        logging.info(f'Creating dataset with {len(self.ids)} examples')\n","\n","    def __len__(self):\n","        return len(self.ids)\n","\n","    @staticmethod\n","    def preprocess(pil_img, size, is_mask):\n","        newW, newH = size, size\n","        pil_img = pil_img.resize((newW, newH), resample=Image.NEAREST if is_mask else Image.BICUBIC)\n","        img_ndarray = np.asarray(pil_img)\n","        \n","        if is_mask:\n","            img_ndarray = np.apply_along_axis(lambda x: [0, 1] if np.all(x == 255.) else [1, 0], 2, img_ndarray)\n","            img_ndarray = img_ndarray.transpose((2, 0, 1))\n","            # img_ndarray.shape == (2, 256, 256)\n","        else:\n","            img_ndarray = img_ndarray.transpose((2, 0, 1))\n","            # img_ndarray.shape == (3, 256, 256)\n","            img_ndarray = img_ndarray / 255\n","\n","        return img_ndarray\n","\n","    @staticmethod\n","    def load(filename):\n","        ext = splitext(filename)[1]\n","        if ext == '.npy':\n","            return Image.fromarray(np.load(filename))\n","        elif ext in ['.pt', '.pth']:\n","            return Image.fromarray(torch.load(filename).numpy())\n","        else:\n","            return Image.open(filename)\n","\n","    def __getitem__(self, idx):\n","        name = self.ids[idx]\n","        mask_file = list(self.masks_dir.glob(name + self.mask_suffix + '.*'))\n","        img_file = list(self.images_dir.glob(name + '.*'))\n","\n","        assert len(img_file) == 1, f'Either no image or multiple images found for the ID {name}: {img_file}'\n","        assert len(mask_file) == 1, f'Either no mask or multiple masks found for the ID {name}: {mask_file}'\n","        mask = self.load(mask_file[0])\n","        img = self.load(img_file[0])\n","\n","        assert img.size == mask.size, \\\n","            f'Image and mask {name} should be the same size, but are {img.size} and {mask.size}'\n","\n","        img = self.preprocess(img, self.size, is_mask=False)\n","        mask = self.preprocess(mask, self.size, is_mask=True)\n","\n","        if self.transform is not None:\n","            transformed_imgs = self.transform(image=img, mask=mask)\n","            img, mask = transformed_imgs['image'], transformed_imgs['mask']\n","\n","        return {\n","            'image': torch.as_tensor(img.copy()).float().contiguous(),\n","            'mask': torch.as_tensor(mask.copy()).long().contiguous()\n","        }\n","\n","class PLSUDataset(BasicDataset):\n","    def __init__(self, images_dir, masks_dir, size, transform=None):\n","        super().__init__(images_dir, masks_dir, size, mask_suffix='', transform=None)"],"metadata":{"id":"WSgb-1lNN-2-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Data Augmentation"],"metadata":{"id":"qaKPhZ19OE93"}},{"cell_type":"code","source":["import albumentations as A\n","import cv2\n","import os\n","from torch.utils.data import ConcatDataset"],"metadata":{"id":"PAugN-QtODpf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transform = A.Compose([\n","            A.HorizontalFlip(p=1),\n","            A.ShiftScaleRotate(rotate_limit=20, p=0.5, border_mode=cv2.BORDER_CONSTANT),\n","            A.ColorJitter(brightness=0.6, contrast=0.6),\n","            A.CLAHE(p=0.5)\n","        ])"],"metadata":{"id":"-GoLUQltOHpw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dir_img = Path('./PLSU/img')\n","dir_mask = Path('./PLSU/Mask')\n","\n","dataset1 = PLSUDataset(dir_img, dir_mask, size=256, transform=transform)\n","dataset2 = PLSUDataset(dir_img, dir_mask, size=256, transform=transform)\n","dataset3 = PLSUDataset(dir_img, dir_mask, size=256, transform=transform)\n","dataset4 = PLSUDataset(dir_img, dir_mask, size=256, transform=transform)\n","\n","dataset = ConcatDataset([dataset1, dataset2, dataset3, dataset4])"],"metadata":{"id":"qqOYUd9iOJy-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(dataset)"],"metadata":{"id":"s98OI28gOLDc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Model: U-net with Context Fusion Module"],"metadata":{"id":"udGi9_cLOMtc"}},{"cell_type":"code","source":["\"\"\" Parts of the U-Net model \"\"\"\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class DoubleConv(nn.Module):\n","    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n","\n","    def __init__(self, in_channels, out_channels, mid_channels=None):\n","        super().__init__()\n","        if not mid_channels:\n","            mid_channels = out_channels\n","        self.double_conv = nn.Sequential(\n","            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n","            nn.BatchNorm2d(mid_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.double_conv(x)\n","\n","\n","class Down(nn.Module):\n","    \"\"\"Downscaling with maxpool then double conv\"\"\"\n","\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.maxpool_conv = nn.Sequential(\n","            nn.MaxPool2d(2),\n","            DoubleConv(in_channels, out_channels)\n","        )\n","\n","    def forward(self, x):\n","        return self.maxpool_conv(x)\n","\n","\n","class Up(nn.Module):\n","    \"\"\"Upscaling then double conv\"\"\"\n","\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","\n","        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","        self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n","\n","    def forward(self, x1, x2):\n","        x1 = self.up(x1)\n","        # input is CHW\n","        diffY = x2.size()[2] - x1.size()[2]\n","        diffX = x2.size()[3] - x1.size()[3]\n","\n","        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n","                        diffY // 2, diffY - diffY // 2])\n","        # if you have padding issues, see\n","        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n","        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n","        x = torch.cat([x2, x1], dim=1)\n","        return self.conv(x)\n","\n","\n","class OutConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(OutConv, self).__init__()\n","        self.conv_sigmoid = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n","            nn.Sigmoid()\n","        )\n","        \n","\n","    def forward(self, x):\n","        return self.conv_sigmoid(x)"],"metadata":{"id":"WrQrfnQFOODo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ContextFusion(nn.Module):\n","    def __init__(self, channels):\n","        super().__init__()\n","        self.context_modeling = nn.Sequential(\n","            nn.Conv2d(channels, channels, kernel_size=1),\n","            nn.Softmax2d()\n","        )\n","        self.context_transform1 = nn.Sequential(\n","            nn.Conv2d(channels, channels, kernel_size=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(channels, channels, kernel_size=1),\n","            nn.Sigmoid()\n","        )\n","        self.context_transform2 = nn.Sequential(\n","            nn.Conv2d(channels, channels, kernel_size=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(channels, channels, kernel_size=1)\n","        )\n","\n","    def forward(self, x):\n","        x1 = nn.MaxPool2d(2)(x)\n","        x2 = self.context_modeling(x1) * x1\n","        return self.context_transform1(x2) * x1 + self.context_transform2(x2)"],"metadata":{"id":"W0ks77E0Oe_f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class UNet(nn.Module):\n","    def __init__(self, n_channels, n_classes):\n","        super(UNet, self).__init__()\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","\n","        self.inc = DoubleConv(n_channels, 64)\n","        self.down1 = Down(64, 128)\n","        self.down2 = Down(128, 256)\n","        self.down3 = Down(256, 512)\n","        self.cfm = ContextFusion(512)\n","        self.up1 = Up(1024, 512 // 2)\n","        self.up2 = Up(512, 256 // 2)\n","        self.up3 = Up(256, 128 // 2)\n","        self.up4 = Up(128, 64)\n","        self.outc = OutConv(64, n_classes)\n","\n","    def forward(self, x):\n","        x1 = self.inc(x)\n","        x2 = self.down1(x1)\n","        x3 = self.down2(x2)\n","        x4 = self.down3(x3)\n","        x5 = self.cfm(x4)\n","        x = self.up1(x5, x4)\n","        x = self.up2(x, x3)\n","        x = self.up3(x, x2)\n","        x = self.up4(x, x1)\n","        logits = self.outc(x)\n","        return logits"],"metadata":{"id":"1r2-9pRTOg_4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Evaluate"],"metadata":{"id":"V_zfnAGeOj16"}},{"cell_type":"code","source":["def F1Score(y_pred, y_true):\n","    epsilon = 1e-7\n","    tp = (y_true * y_pred).sum()\n","    tn = ((1 - y_true) * (1 - y_pred)).sum()\n","    fp = ((1 - y_true) * y_pred).sum()\n","    fn = (y_true * (1 - y_pred)).sum()\n","\n","    precision = tp / (tp + fp + epsilon)\n","    recall = tp / (tp + fn + epsilon)\n","        \n","    return 2* (precision*recall) / (precision + recall + epsilon)\n","\n","def IOUScore(y_pred, y_true):\n","    epsilon = 1e-7\n","\n","    intersection = (y_pred & y_true).sum()\n","    union = (y_pred | y_true).sum()\n","    iou = (intersection + epsilon) / (union + epsilon)\n","    thresholded = np.ceil(np.clip(20 * (iou - 0.5), 0, 10)) / 10\n","\n","    return thresholded"],"metadata":{"id":"OCXqJa--Oix7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(net, dataloader, device):\n","    net.eval()\n","    num_val_batches = len(dataloader)\n","    f1_score = 0\n","    iou_score = 0\n","    # iterate over the validation set\n","    with tqdm(total=num_val_batches, desc='Validation round', unit='batch', leave=False) as pbar:\n","        for batch in dataloader:\n","            image, mask_true = batch['image'], batch['mask']\n","            # move images and labels to correct device and type\n","            image = image.to(device=device, dtype=torch.float32)\n","\n","            with torch.no_grad():\n","                # predict the mask\n","                mask_pred = net(image) # shape = (64, 2, 256, 256)\n","\n","                y_true = np.apply_along_axis(lambda x: 1 if x[1] == 1 else 0, 3, mask_true) # shape = (64, 256, 256)\n","\n","                y_pred = mask_pred.permute(0,2,3,1).detach().cpu().numpy() # shape = (64, 256, 256, 2)\n","                y_pred = np.apply_along_axis(lambda x: 1 if x[0] < x[1] else 0, 3, y_pred) # shape = (64, 256, 256)\n","\n","                f1 = F1Score(y_pred, y_true).mean()\n","                iou = IOUScore(y_pred, y_true).mean()\n","                \n","                f1_score += f1\n","                iou_score += iou\n","\n","                pbar.set_postfix(**{'F1 Score (batch)': f1, 'IOU Score (batch)': iou})\n","\n","    net.train()\n","\n","    return f1_score/num_val_batches, iou_score/num_val_batches"],"metadata":{"id":"Ch_P2JIqOmdn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Train"],"metadata":{"id":"EpszJYmVOo3h"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, random_split\n","from torch.optim import Optimizer\n","from tqdm import tqdm\n","import torch.nn.functional as F"],"metadata":{"id":"2_IafhRgOp0c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_percent = 0.1\n","test_percent = 0.1\n","batch_size = 64\n","epochs = 100"],"metadata":{"id":"1hKa-Hd8OrK7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_val = int(len(dataset) * val_percent)\n","n_test = int(len(dataset) * test_percent)\n","n_train = len(dataset) - n_val - n_test\n","train_set, val_set, test_set = random_split(dataset, [n_train, n_val, n_test], generator=torch.Generator().manual_seed(0))"],"metadata":{"id":"1xp3sA0bOst4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loader_args = dict(batch_size=batch_size, num_workers=2, pin_memory=True) # maximal two workers\n","train_loader = DataLoader(train_set, shuffle=True, **loader_args)\n","val_loader = DataLoader(val_set, shuffle=False, drop_last=True, **loader_args)"],"metadata":{"id":"ueKg9o7POuKT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net = UNet(n_channels=3, n_classes=2).to(DEVICE)\n","optimizer = torch.optim.Adam(net.parameters(), lr=0.0001, weight_decay=0.00001)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=8)\n","criterion = nn.CrossEntropyLoss()\n","grad_scaler = torch.cuda.amp.GradScaler(enabled=True)\n","global_step = 0\n","patience_stop = 10\n","patience = 0\n","f1_scores = []\n","iou_scores = []\n","epoch_losses = []\n","best_loss = 10 ** 9"],"metadata":{"id":"ixRMl7qMOvhj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(1, epochs+1):\n","        net.train()\n","        epoch_loss = 0\n","        with tqdm(total=n_train, desc=f'Epoch {epoch}/{epochs}', unit='img') as pbar:\n","            for batch in train_loader:\n","                images = batch['image']\n","                true_masks = batch['mask']\n","\n","                assert images.shape[1] == net.n_channels\n","\n","                # Train round\n","                images = images.to(device=DEVICE, dtype=torch.float32) # shape = (64, 3, 256, 256)\n","                true_masks = true_masks.to(device=DEVICE, dtype=torch.float32) # shape = (64, 2, 256, 256)\n","\n","                with torch.cuda.amp.autocast(enabled=True):\n","                    masks_pred = net(images) # shape = (64, 2, 256, 256)\n","                    loss = criterion(masks_pred, true_masks)\n","\n","                optimizer.zero_grad(set_to_none=True)\n","                grad_scaler.scale(loss).backward()\n","                grad_scaler.step(optimizer)\n","                grad_scaler.update()\n","\n","                pbar.update(images.shape[0])\n","                global_step += 1\n","                epoch_loss += loss.item()\n","\n","                pbar.set_postfix(**{'loss (batch)': loss.item()})\n","\n","        # an epoch finished\n","        epoch_losses.append(epoch_loss)\n","\n","        # schedule learning rate\n","        scheduler.step(epoch_loss)\n","        f1, iou = evaluate(net, val_loader, DEVICE)\n","        f1_scores.append(f1)\n","        iou_scores.append(iou)\n","\n","        # early stop\n","        if epoch_loss > best_loss:\n","            patience += 1\n","            if patience_stop == patience:\n","                print(f\"early stopped in epoch={epoch}\")\n","                break\n","            else:\n","                best_loss = epoch_loss\n","                patience = 0"],"metadata":{"id":"frbPpdFSOyLI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Test"],"metadata":{"id":"4EBO-J24O-dl"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from torchvision.utils import save_image, make_grid\n","\n","test_loader = DataLoader(test_set, shuffle=False, batch_size=1)"],"metadata":{"id":"JN4qO25HO9JL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for data in test_loader:\n","    img = data['image']\n","    mask = data['mask']\n","    with torch.no_grad():\n","        img = img.to(device=DEVICE, dtype=torch.float32)\n","        mask = mask.to(device=DEVICE, dtype=torch.float32)\n","        output = net(img)\n","        print(output.shape)\n","        # output = torch.Tensor(np.apply_along_axis(lambda x: [1,1,1] if x[0] > x[1] else [0,0,0], 1, output.cpu().detach()))\n","        # plt.imshow(make_grid(output.squeeze().permute((1,2,0))))\n","        # plt.show()"],"metadata":{"id":"uW5y-bqlPAnD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for batch in dataset:\n","    print(batch['mask'].shape)\n","    true = np.apply_along_axis(lambda x: 1 if x[0] < x[1] else 0, 2, batch['mask'])\n","    print(true.shape)\n","    pred = net(batch['image'].unsqueeze(0).cuda())[0].permute(1,2,0)\n","    pred = np.apply_along_axis(lambda x: 1 if x[0] < x[1] else 0, 2, pred.detach().cpu().numpy())\n","    # print(F1Score(pred, true))\n","    \n","    # print(pred.detach().numpy().shape, batch['mask'].numpy().shape)\n","    print(IOUScore(true, true))\n","    mask = np.apply_along_axis(lambda x: [255, 255, 255] if x[1] == 1 else [0, 0, 0], 2, batch['mask'])\n","    plt.imshow(mask)\n","    plt.show()"],"metadata":{"id":"Gq5UW3rmPGIt"},"execution_count":null,"outputs":[]}]}